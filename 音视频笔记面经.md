# 音视频



## **流媒体学习框架图**

![图片](https://mmbiz.qpic.cn/mmbiz_png/GjBg4Rhq9j1blIicbib2zA6xLUaoYJw7xvUjvJJOPPuuibW5EmibhHlWXHBCYrGUmy5eAxA3rcGZaUG9DGuWTXSpeQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)





## 音视频播放原理

​		音视频播放的分类，直观应用上，音视频播放主要分为两大类：在线播放和本地播放。在线播放即通过互联网，在线播放音视频；本地播放及播放本地存放的音视频文件。

​		音视频播放的原理主要分为：**解协议->解封装->解码->音视频同步->播放**。当然，如果是本地播放，没有解协议这一步骤。

​		音视频播放的原理如下（大家可以想象成**剥洋葱一样**，音视频播放其实**是一层层的去除协议、封装，再解码，最后得到原始数据）：**

![](.\assets\1.png)





## 录播、点播、直播

​		**录播**：录播更侧重于“录”，比如录播系统，主要集成了音视频的采集、后期剪辑、工具软件的系统。通俗的讲录播就是**生产音视频**。



​		**点播**：点播从字面意义上讲是播放选择的视频，比如观看爱奇艺、腾讯等视频网站的电影和综艺，**可以随意拖动视频进度**，这些音视频共性特点是**提前录制好的**。通俗的讲点播就是播放**录制好的视频**，点播是**消费音视频**。



​		**直播**：直播相对好理解，虎牙、斗鱼等直播平台上的游戏、才艺直播，这些音视频共性特点是实时的，观看者**不能拖动音视频进度**。通俗的讲直播就是播放**实时直播视频**，直播**既生产又消费音视频**。





## 图像篇（YUV和RGB）

​		YUV，RGB，包括YcbCr是色彩空间的模型，而平常所说的BMP、PNG、JPEG是文件的存储形式。而YUV是音视频（**编解码**）最常用的格式。

YUV(简单来说是指**亮度、色相、饱和度**，下面的表达即指像素点对这三个值的存储。

YUV是一种颜色编码系统，通常用于数字视频和图像处理。**它将亮度（Y）与色度（UV）分离，使其更适合于视频压缩和传输**。在YUV中：

1. Y代表亮度（Luma），表示图像的灰度信息或亮度信息。
2. U和V代表色度（Chrominance），表示颜色信息。U表示蓝色与亮度之间的差异，V表示红色与亮度之间的差异。

YUV通常用于视频编码和视频压缩中，其中**分离亮度和色度可以更有效地压缩数据**，**因为人眼对亮度变化更敏感**，**而对色度变化不太敏感**。



**1.YUV的数据格式是如何呢？**

​		YUV有两种分类方式，即“空间-间”和“空间-内”。“空间-间”的划分方式主要体现在Y、U、V的比例不同；“空间-内”的划分方式主要体现在Y、U、V的比例一定，存储格式不同。

**YUV的两种分类方式， 空间-间，主要体现在Y、U、V的比例不同；空间-内则体现在Y、U、V的比例相同。**



**2.YUV“空间-间”的数据划分**

​		YUV按照“空间-间”的划分方式，分为YUV444、YUV422、YUV420，如下所示，假设图像为1920*1080：

下图中，**每一个[]代表一个像素点**,YUV444中的数字代表**字节比例** 



### YUV444

![image-20240323100328574](C:\Users\Aoeivuxt\AppData\Roaming\Typora\typora-user-images\image-20240323100328574.png)



 		色度信号分辨率最高的格式是YUV4:4:4，**每4点Y采样，就有相对应的4点U和4点V**。换句话说，每个Y值对应一个U和一个V值。在这种格式中，色度信号的分辨率和亮度信号的分辨率是相同的。这种格式**主要应用在视频处理设备内部**，**避免画面质量在处理过程中降低**。

![image-20240323101037259](.\assets\image-20240323101037259.png)



​		一种简单的YUV444区分方法：如图2所示，4个Y值，第1行获得4组UV色度值（如像素1、2、3、4均可独立表示），第2行也获得4组UV色度值（如像素5、6、7、8均可独立表示）。

### YNV422

![image-20240323100340396](.\assets\image-20240323100340396.png)

 		色度信号分辨率格式YUV4:2:2，**每4点Y采样，就有相对应的2点U和2点V**。可以看到在水平方向上的色度表示进行了2倍下采样，因此YUV422色度信号分辨率是亮度信号分辨率的一半。

![image-20240323101246329](.\assets\image-20240323101246329.png)

​		一种简单的YUV422区分方法：如图3所示，4个Y值，第1行获得2组UV色度值（其中像素1、2合并为1组UV值表示，3、4合并为1组），第2行获得2组UV色度值（其中像素5、6合并为1组，7、8合并为1组)。



### YUV420



​		色度信号分辨率格式YUV4:2:0，**每4点Y采样，就有相对应的1点U和1点V**。YUV420色度信号分辨率是亮度信号分辨率的1/4。

![image-20240323100349169](.\assets\image-20240323100349169.png)

![image-20240323101415070](.\assets\image-20240323101415070.png)	

 		一种简单的YUV420区分方法：如上图所示，4个Y值，第1行获得2组UV色度值（其中1、2合并为1组，3、4合并为1组），第2行获得0组UV色度值（5-8像素的色度值全丢弃）。即在水平方向压缩的基础上，再在垂直方向上再进行了压缩。



### **YUV不同采样格式对图像画质的影响分析**

​		根据前述的YUV采样格式分析，这里我们分析一下对图像画质的影响。我们将一个原始图像为8*8像素的红蓝相间的图案，分别按YUV444、YUV422、YUV420不同的采用格式采样，然后再还原输出。

​		图5(a)：我们可以看到YUV444的色度信号的分辨率和亮度信号的分辨率无损失，我们获得了与原始图案一致的还原画面图案。

​		图5(b)：YUV422获得还原图案在水平方向上，已经出现了丢失，从绿色所框选的像素来看，YUV422在水平方向上丢失了另一个像素点的色彩值，故在画面还原时仅是对前一个像素值简单的复制重构。

​		图5(c)：YUV420获得还原图案在水平方向以及垂直方向上，均出现了丢失，获得的还原图像与原始图像出现很大的失真。



![image-20240323101556216](.\assets\image-20240323101556216.png)



​	**总结**

​		在**图像高频细节的图像表达上**，YUV444优于YUV422，YUV422优于YUV420。

​		在**信号传输带宽的节省上**，YUV420效率优于YUV444，YUV422优于YUV444。因此在普通的视频编解码算法上，为节省传输带宽开销，普遍采用YUV420或者YUV422的采样格式。



### YUV和RGB的转换公式

RGB：即red，green，blue三色存储空间，因音视频主要用的是YUV的色彩空间，RGB和YUV的转换公式：

◆ RGB 转 YUV：

  Y = 0.299R + 0.587G + 0.114B

  U= -0.147R - 0.289G + 0.436B

  V = 0.615R - 0.515G - 0.100B

◆ YUV 转 RGB：

  R = Y + 1.14V

  G = Y - 0.39U - 0.58V

  B = Y + 2.03U









## H.264

H.264作为现在应用比较广泛的视频编码格式标准，本文笔者介绍下H.264相关知识。



**1.什么是H.264**

H.264是由ITU-T视频编码专家组（VCEG）和ISO/IEC动态图像专家组（MPEG）联合组成的联合视频组（JVT，Joint Video Team）提出的**高度压缩数字视频编解码器标准**。



**2.H.264的数据格式是怎样的？**

H.264由**视频编码层（VCL）**和**网络适配层（NAL）**组成。

◆ VCL：**H264编码/压缩的核心**，**主要负责将视频数据编码/压缩，再切分**。

◆ NALU = NALU header + NALU payload



**3.VCL是如何管理H264视频数据？**

◆ **压缩**：预测（帧内预测和帧间预测）-> DCT变化和量化 -> 比特流编码；

◆ **切分数据**，主要为了第三步。"切片(slice)"、“宏块(macroblock)"是在VCL中的概念，一方面提高编码效率和降低误码率、另一方面提高网络传输的灵活性。

◆ **包装成『NAL』**。

综上所述**『VCL』最后会被包装成『NAL』, 也就是『VCL』被转化为『NAL』**

 **4.NAL头的数据结构体**

![image-20240323103609978](.\assets\image-20240323103609978.png)

◆ F（forbidden_zero_bit）：1 位，初始为0。当网络识别此单元存在比特错误时，可将其设为 1，以便接收方丢掉该单元

◆ NRI（nal_ref_idc）：2 位，用来指示该NALU 的重要性等级。值越大，表示当前NALU越重要。具体大于0 时取何值，没有明确规定

◆ Type（nal_unit_type）：5 位，指出NALU 的类型，如下所示：



![image-20240323104156452](.\assets\image-20240323104156452.png)

**5.H.264码流结构**

◆ H.264 = start_code + NALU（start_code：**00000001** or **000001**）



![image-20240323104215663](.\assets\image-20240323104215663.png)





◆ 每个NAL前有一个起始码 0x00 00 01（或者0x00 00 00 01），解码器检测每个起始码，作为一个NAL的起始标识，当检测到下一个起始码时，当前NAL结束。

◆ 同时H.264规定，当检测到0x000000时，也可以表征当前NAL的结束。那么NAL中数据出现0x000001或0x000000时怎么办？H.264引入了防止竞争机制，如果编码器检测到NAL数据存在**0x000001**或**0x000000**时，编码器会在最后个字节前插入一个新的字节0x03，这样：

0x000000－>0x00000300

0x000001－>0x00000301

0x000002－>0x00000302

0x000003－>0x00000303



**6.I帧、P帧和B帧**

提到H.264，不得不提I帧、P帧、B帧、IDR帧、GOP。

◆ I帧（Intra-coded picture，帧内编码图像帧），表示关键帧，采用类似JPEG压缩的DCT(Discrete Cosine Transform，离散余弦变换)压缩技术，可达1/6压缩比而无明显压缩痕迹；

◆ P帧（Predictive-coded picture，前向预测编码图像帧），表示的是跟之前的一个关键帧或P帧的差别，P帧是参考帧，它可能造成解码错误的扩散；

◆ B帧（Bidirectionally predicted picture，双向预测编码图像帧），本帧与前后帧（I或P帧）的差别，B帧压缩率高，但解码耗费CPU；

◆ IDR帧（Instantaneous Decoding Refresh，即时解码刷新）：首个I帧，是立刻刷新,使错误不致传播，IDR导致DPB（DecodedPictureBuffer参考帧列表——这是关键所在）清空；在IDR帧之后的所有帧都不能引用任何IDR帧之前的帧的内容；IDR具有随机访问的能力，播放器可以从一个IDR帧播放。

◆ GOP（Group Of Picture，图像序列）：两个I帧之间是一个图像序列，一个GOP包含一个I帧



**7.解码时间戳和显示时间戳**

当然，H.264中还有两个重要的概念：DTS和PTS

◆ DTS（Decoding Time Stamp，解码时间戳解）：读入内存中的比特流在什么时候开始送入解码器中进行解码

◆ PTS（Presentation Time Stamp，显示时间戳）：解码后的视频帧什么时候被显示出来



![image-20240323104540713](.\assets\image-20240323104540713.png)





## PCM

说到音频，主要有两个概念比较重要，一个是**采集到的原始音频数据（比如PCM**）和**压缩后的音频数据（比如AAC）**。



**1.什么是PCM？**

PCM(Pulse Code Modulation，脉冲编码调制)音频数据是未经压缩的音频采样数据裸流，它是由模拟信号经过采样、量化、编码转换成的标准数字音频数据。



**2.如何理解PCM？**

PCM是一种用数字表示采样模拟信号方法。主要包括采样，量化，编码三个主要过程。

 ◆ 先来看看模拟信号采样的过程：

![image-20240323104708577](.\assets\image-20240323104708577.png)



 ◆采样率：每秒钟采样的样本数。比如我们常说的44.1kHz，即每秒钟采样44100次。

 ◆ 量化：将采样信号数据四舍五入到一个可用整数表示的过程。（位深）

![image-20240323104735501](.\assets\image-20240323104735501.png)



 ◆ 编码：将量化后的信号转换成二进制数据。



**3.描述PCM数据的6个参数：**

 ◆ Sample Rate : 采样频率。8kHz(电话)、44.1kHz(CD)、48kHz(DVD)。

 ◆ Sample Size : 量化位数。常见值为8-bit、16-bit。

 ◆ Number of Channels : 通道个数。常见的音频有立体声(stereo)和单声道(mono)两种类型，立体声包含左声道和右声道。另外还有环绕立体声等其它不太常用的类型。

 ◆ Sign : 表示样本数据是否是有符号位，比如用一字节表示的样本数据，有符号的话表示范围为-128 ~ 127，无符号是0 ~ 255。

 ◆ Byte Ordering : 字节序。字节序是little-endian还是big-endian。通常均为little-endian。

 ◆ Integer Or Floating Point : 整形或浮点型。大多数格式的PCM样本数据使用整形表示，而在一些对精度要求高的应用方面，使用浮点类型表示PCM样本数据。



## ACC

**1.什么是AAC？**

AAC(Advanced Audio Coding，高级音频编码)是一种声音数据的文件压缩格式。AAC分为**ADIF**和**ADTS**两种文件格式。

**2.什么是ADIF和ADTS？**

 ◆ ADIF：Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个**音频数据的开始**，不需进行在音频数据流中间开始的解码，即它的解码必须在**明确定义的开始处**进行。故这种格式常用在磁盘文件中。

 ◆ ADTS：Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个**有同步字的比特流**，解码可以在这个流中任何位置开始。



**3.ADTS的数据结构是怎样的？**

![image-20240402162200643](.\assets\image-20240402162200643.png)



## SSA & ASS

常见的字幕有srt、ssa、ass等格式，本文主要介绍下ssa和ass的格式。



**1.什么是SSA？**

◆ SSA（SubStation Alpha），是由CS Low（亦称Kotus）创建，比传统字幕格式（如SRT）功能更加先进的字幕文件格式。

◆ 该格式字幕的外挂文件以*.ssa作为后缀。

**2.什么是ASS？**

◆ ASS（Advanced SubStation Alpha），是一种比SSA更为高级的字幕格式, 其实质版本是SSA v4.00+，它是基于SSA 4.00+编码构建的。

◆ ASS的主要变化就是在SSA编写风格的基础上增添更多的特效和指令。

◆ 该格式字幕的外挂文件以*.ass作为后缀。



**3.SSA/ASS的基本结构**

SSA/ASS字幕是一种类ini风格纯文本文件；包含五个section：[Script Info]、[v4+ Styles]、[Events]、[Fonts]、[Graphics]。

◆ [Script Info]：包含了脚本的头部和总体信息。[Script Info] 必须是 v4 版本脚本的第一行。

◆ [v4 Styles]：包含了所有样式的定义。每一个被脚本使用的样式都应该在这里定义。ASS 使用 [v4+ Styles]。

◆ [Events]：包含了所有脚本的事件，有字幕、注释、图片、声音、影像和命令。基本上，所有在屏幕上看到的内容都在这一部分。

◆ [Fonts]：包含了脚本中内嵌字体的信息。

◆ [Graphics]：包含了脚本中内嵌图片的信息。



**4.SSA字幕范例**

SSA字幕样本范例如下：

![image-20240402162615516](.\assets\image-20240402162615516.png)



## FLV

**1.FLV的封装格式**

FLV（Flash Video），Adobe公司设计开发的一种流行的流媒体格式，由于其**视频文件体积轻巧**、封装简单等特点，使其很适合在互联网上进行应用。除了**播放视频**，在直播时也可以使用。采用FLV格式封装的文件后缀为.flv，格式如下（FLV = FLV Header + Body）：

![image-20240430185746154](./assets/image-20240430185746154.png)





**2.FLV Header**

Header 部分记录了FLV的**类型**、**版本**、**流信息**、**Header 长度**等。一般整个Header占用9个字节，大于9个字节则表示头部信息在这基础之上还存在扩展数据。FLV Header 的信息排布如下所示：

![image-20240430185810699](./assets/image-20240430185810699.png)



**3.FLV Body**

**Body 是由一个个Tag组成的**，每个Tag下面有一块4个字节的空间，用来记录这个Tag 的长度。这个后置的PreviousTagSize用于逆向读取处理，表示的是前面的Tag的大小。FLV Body 的信息排布如下：

![image-20240430185819941](./assets/image-20240430185819941.png)



**4.FLV Tag**

每个Tag 也是由两部分组成的：**Tag Header** 和 Tag Data。Tag Header 存放了**当前Tag的类型，数据长度、时间戳、时间戳扩展、StreamsID等信息**，然后再接着数据区Tag Data。Tag的排布如下：

![](./assets/image-20240430185843116.png)



**5.Tag Data**

Tag Data分成 Audio，Video，Script 三种。

**5.1 Audio Tag Data**

音频的Tag Data又分为 AudioTagHeader 和 Data 数据区，其排布结构如下图所示：



![](./assets/image-20240430185850134.png)

**5.2 Video Tag Data**

◆ Video Tag 由一个字节的VideoTagHeader 和 Video数据区部分组成

![](./assets/image-20240430185854956.png)

◆ Video数据区部分格式不确定。对于H264/AVC编码部分，Video数据区排布如下:

![](./assets/image-20240430185903219.png)



**5.3 Script Tag Data**



![图片](https://mmbiz.qpic.cn/mmbiz_png/GjBg4Rhq9j1ibIeFvpFDJbibUQOAR2FLHgwoVW0Mic81flQTjqbJX1AaY6C0ovpmxVFE1FFs4J86RHtc510JaFAng/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)





# 音视频面经



## 用 UDP 实现音视频，有什么方法可以保证通话质量？

使用 UDP 享受了低延时，牺牲了可靠性。但可靠性牺牲太多导致不可用也是不可接受的，所以还需要做一些机制来保证一定的可靠性，比如我们可以参考 WebRTC 的机制：

- NACK：通过丢包重传解决丢包问题，会增加延时。
- FEC：通过冗余数据解决丢包问题，会增加带宽占用。
- JitterBuffer：通过队列对接收到的数据进行缓冲，出队时将数据包均匀平滑的取出，解决视频的乱序与抖动。
- NetEQ：类似 JitterBuffer，解决音频的乱序与抖动。





## CDN 在直播中有哪些运用?

CDN 的全称为 Content Delivery Network，即内容分发网络，是一个策略性部署的整体系统，主要用来解决由于网络带宽小、用户访问量大、网点分布不均匀等导致用户访问网站速度慢的问题。这中间就有了很多的 CDN 节点。

具体实现是通过在现有的网络中，增加一层新的网络架构，将网站的内容发布到离用户最近的网络节点上，这样用户可以就近获取所需的内容，解决之前网络拥塞、访问延迟高的问题，提高用户体验。



## RTMP 消息分优先级的设计有什么好处？

RTMP 的消息优先级是：控制消息 > 音频消息 > 视频消息。当网络传输能力受限时，优先传输高优先级消息的数据。

要使优先级能够有效执行，分块也很关键：将大消息切割成小块，可以避免大的低优先级的消息（如视频消息）堵塞了发送缓冲从而阻塞了小的高优先级的消息（如音频消息或控制消息）。





## 什么是 DTS 和 PTS？它们有什么区别？

DTS 是解码时间戳；PTS 是显示时间戳。

虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。

当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，就回到了我们前面说的问题：解码顺序和播放顺序不一致了。DTS 告诉我们该按什么顺序解码这几帧图像，PTS 告诉我们该按什么顺序显示这几帧图像。



## 什么是 IDR 帧？它和 I 帧有什么区别？

IDR 帧全称叫做 Instantaneous Decoder Refresh，是 I 帧的一种。IDR 帧的作用是立刻刷新，重新算一个新的序列开始编码，使错误不致传播。







## TCP协议-如何保证传输可靠性?



TCP协议保证数据传输可靠性的方式主要有：

- **停止等待协议**
          每发送完一个分组，就停止发送，等待对方确认，收到确认后再发送下一个分组。
- **连续ARQ协议**
          利用滑动窗口，位于滑动窗口内的所有分组都可以连续的发送出去，而不需要逐个等待对方的确认。A每收到一个确认，就把发送窗口**向前滑动一个分组**的位置。B采用累积确认的方式，对按序到达的最后一个分组发送确认（就是最后这个分组的编号），就表示**这个分组之前的所有分组都收到**了。
- **校验和**
- **序列号**
- **确认应答**
- **超时重传**
- **连接管理**
- **流量控制**
- **拥塞控制**



## 线程池讲一下，需要注意哪些参数？怎么创建线程池比较好？

​		线程池是一种管理和复用线程的机制，它可以在程序启动时创建一组线程，并且根据需要**动态地调整线程数量**，从而更有效地管理系统资源和提高程序的性能。线程池通常包含了**一个线程队列**和**一组线程**，当有任务需要执行时，线程池从队列中取出一个线程来执行任务，任务执行完毕后线程返回到线程池中等待下一个任务。





在创建线程池时，需要注意以下几个重要参数：

1. **核心线程数（corePoolSize）**：
   - 核心线程数指定了线程池中保持活动状态的线程数量，即使没有任务需要执行，核心线程也会一直存活。
   - 核心线程通常用于执行短期的、频繁出现的任务，以减少线程的创建和销毁开销。
2. **最大线程数（maximumPoolSize）**：
   - 最大线程数指定了线程池中允许存在的最大线程数量，当任务队列已满且活动线程数达到最大线程数时，新的任务将会被拒绝。
   - 最大线程数应该根据系统资源和任务特性来合理设置，避免因线程数过多导致系统资源耗尽。
3. **任务队列（workQueue）**：
   - 任务队列用于存放提交的任务，等待线程池中的线程来执行。
   - 不同类型的任务队列有不同的特性，例如 `ArrayBlockingQueue`、`LinkedBlockingQueue`、`SynchronousQueue` 等，选择适合应用场景的任务队列非常重要。
4. **线程存活时间（keepAliveTime）和线程存活时间单位（unit）**：
   - 当线程池中的线程数量超过核心线程数时，空闲的线程在经过一段时间后会被销毁，以释放系统资源。
   - 线程存活时间指定了空闲线程的最大存活时间，单位为毫秒、秒、分钟等。
5. **拒绝策略（rejectedExecutionHandler）**：
   - 当任务队列已满且线程池中的线程数量达到最大线程数时，新提交的任务将会被拒绝。
   - 拒绝策略定义了当线程池无法接受新任务时的处理方式，常见的策略包括抛出异常、丢弃任务、调用提交任务的线程来执行任务等。

​		要创建一个线程池，可以使用 `java.util.concurrent` 包中的 `ThreadPoolExecutor` 类，它是 `Executor` 接口的一个具体实现，提供了丰富的参数和灵活的配置选项。通常情况下，可以根据应用程序的特性和需求来调整线程池的参数，以达到最佳的性能和资源利用率。







## wait/sleep的区别?

整体的区别其实是有四个:

1、所属类不同:sleep是**线程**中的方法，但是wait是**Object**中的方法。

2、语法不同:sleep方法不依赖于同步器synchronized，但是**wait需要依赖synchronized**关键字

3、参数不同:sleep必须设置参数时间，wait可以不设置时间，不设置将一直休眠。

4、释放锁资源不同:sleep方法不会释放lock，但是wait会释放，而且会加入到等待队列中

5、唤醒方式不同:sleep不需要被唤醒**(休眠之后推出阻塞**)，但是wait需要(不指定时间**需要被别人中断**)。

6、线程进入状态不同:调用 seep 方法线程会进入 TIMED_WAITING 有时限等待状态，而调用无参数的 wait 方法，线程会进入 WAITING 无时限等待状态。

